{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "759fe523",
   "metadata": {},
   "source": [
    "# Optimiseur ADAGRAD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b58d30b",
   "metadata": {},
   "source": [
    "## I.\tPrésentation de l'algorithme Adagrad\n",
    "L'algorithme Adagrad est une méthode d'optimisation des algorithmes d'apprentissage automatique, principalement utilisée pour l'optimisation des paramètres d'un modèle dans le cadre de l'apprentissage profond. Il a été introduit par John Duchi, Elad Hazan et Yoram Singer en 2011.\n",
    "L'objectif principal d'Adagrad est d'ajuster les taux d'apprentissage des différents paramètres du modèle de manière adaptative, en prenant en compte leur comportement passé. Contrairement à certaines méthodes d'optimisation qui utilisent un taux d'apprentissage global constant, Adagrad adapte le taux d'apprentissage pour chaque paramètre individuellement.\n",
    "En résumé, Adagrad est un algorithme d'optimisation adaptatif qui ajuste les taux d'apprentissage des paramètres en fonction de l'historique des mises à jour des gradients. Il est couramment utilisé dans les méthodes d'apprentissage profond et a permis d'améliorer les performances de nombreux modèles en permettant une adaptation plus fine des taux d'apprentissage.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "90ff15ee",
   "metadata": {},
   "source": [
    "## II.\tFonctionnement de l'algorithme Adagrad\n",
    "L'algorithme Adagrad est une méthode d'optimisation utilisée pour entraîner des modèles d'apprentissage automatique, en particulier dans le domaine de l'apprentissage profond. Il s'agit d'une variante de la descente de gradient stochastique (SGD) qui adapte automatiquement le taux d'apprentissage pour chaque poids du modèle, en fonction de l'historique des gradients précédents.\n",
    "Voici le fonctionnement de l'algorithme Adagrad en quelques étapes :\n",
    "Initialisation : Tout d'abord, les poids du modèle sont initialisés avec des valeurs aléatoires ou avec une autre méthode d'initialisation. De plus, une variable d'accumulation des carrés des gradients, appelée \"accumulateur\", est initialisée pour chaque poids. L'accumulateur est une matrice diagonale qui stocke la somme des carrés des gradients précédents pour chaque poids.\n",
    "Calcul du gradient : Lors de chaque itération de l'entraînement, un lot (batch) d'exemples d'entraînement est utilisé pour calculer le gradient de la fonction de perte par rapport aux poids du modèle. Le gradient mesure la direction et l'amplitude du changement à apporter aux poids pour minimiser la perte.\n",
    "Mise à jour des accumulateurs : Une fois le gradient calculé, les accumulateurs sont mis à jour. Chaque élément de l'accumulateur est augmenté en ajoutant le carré du gradient correspondant. Cela signifie que plus un poids est mis à jour fréquemment avec des gradients importants, plus son accumulateur sera élevé.\n",
    "Mise à jour des poids : Ensuite, les poids du modèle sont mis à jour en utilisant la formule suivante pour chaque poids : nouveau poids = poids actuel - (taux apprentissage / racine carrée (accumulateur + epsilon)) * gradient\n",
    "$$W_{i+1}\\rightarrow W_{i} - \\frac{Lr}{\\sqrt{Accumulateur + \\epsilon}} * Gradients$$\n",
    "où taux apprentissage est un hyper paramètre qui contrôle la taille des pas de mise à jour des poids, racine carrée représente la fonction de racine carrée, accumulateur est la valeur actuelle dans l'accumulateur pour le poids correspondant, epsilon est un terme de régularisation pour éviter la division par zéro et gradient est la valeur du gradient calculée précédemment.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88bbb1b6",
   "metadata": {},
   "source": [
    "## III.\tAvantages et inconvénients d'Adagrad\n",
    "- Avantages :  \n",
    "Le taux d'apprentissage change pour chaque paramètre d'entraînement.  Adaptation des taux d'apprentissage : Adagrad ajuste automatiquement les taux d'apprentissage pour chaque paramètre du modèle en fonction de leur historique des gradients. Cela permet de mettre à jour les paramètres avec des taux d'apprentissage plus élevés pour les paramètres moins fréquemment mis à jour et des taux d'apprentissage plus faibles pour les paramètres fréquemment mis à jour. Cela peut aider à obtenir une convergence plus rapide et une meilleure optimisation.  Pas besoin de régler le taux d'apprentissage manuellement : Avec Adagrad, il n'est généralement pas nécessaire de régler manuellement le taux d'apprentissage. L'optimiseur adapte automatiquement les taux d'apprentissage pour chaque paramètre, ce qui peut faciliter le processus d'optimisation.  Capable de s'entraîner sur des données rares.     \n",
    "- Inconvénients :\n",
    "Sa principale faiblesse est l'accumulation des gradients au carré ( Gt ) dans le dénominateur. Étant donné que chaque terme ajouté est positif, la somme accumulée continue de croître pendant la formation, ce qui entraîne une diminution du taux d'apprentissage et devient infiniment petit, ce qui conduit à une convergence lente.  Moins efficace que certains autres algorithmes d'optimisation comme AdaDelta et Adam\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1967cbfb",
   "metadata": {},
   "source": [
    "## IV.\tComparaison avec d'autres algorithmes d'optimisation\n",
    "### Descente de gradient stochastique (SGD):\n",
    "- Simplicité\n",
    "- Convergence lente\n",
    "- Sensible aux valeurs initiales des paramètres\n",
    "### Momentum :\n",
    "- Utilise le concept de moment pour accélérer la convergence\n",
    "- Atténue l'impact des fluctuations de gradient\n",
    "- Aide à sortir des minimas locaux et éviter les plateaux\n",
    "### AdaGrad :\n",
    "- Adapte le taux d'apprentissage pour chaque paramètre en fonction de l'historique des gradients\n",
    "- Efficace pour les données rares ou bruitées\n",
    "- Réduit le taux d'apprentissage pour les paramètres avec de grands gradients accumulés\n",
    "### RMSprop :\n",
    "- Variante d'AdaGrad qui résout le problème de l'accumulation des carrés des gradients\n",
    "- Utilise une moyenne mobile des carrés des gradients\n",
    "- Conserve une adaptation dynamique du taux d'apprentissage\n",
    "### Adam :\n",
    "- Combinaison du Momentum et de RMSprop\n",
    "- Estimation adaptative du premier moment (moyenne des gradients) et du second moment non centralisé (moyenne mobile des carrés des gradients)\n",
    "- Adaptation fine du taux d'apprentissage pour chaque paramètre\n",
    "- Convergence rapide, même avec des taux d'apprentissage élevés\n",
    "\n",
    "En résumé, la descente de gradient stochastique est simple mais converge lentement, tandis que les optimiseurs Momentum, AdaGrad, RMSprop et Adam apportent des améliorations spécifiques en termes d'accélération de la convergence, d'adaptation du taux d'apprentissage et de gestion des problèmes potentiels. Le choix de l'optimiseur dépend du problème, des données et des performances recherchées.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ecac9ae",
   "metadata": {},
   "source": [
    "## V.\tExemples d'application "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "86bd242d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importation des bibliothèque\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "69573c80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
       "0            6      148             72             35        0  33.6   \n",
       "1            1       85             66             29        0  26.6   \n",
       "2            8      183             64              0        0  23.3   \n",
       "3            1       89             66             23       94  28.1   \n",
       "4            0      137             40             35      168  43.1   \n",
       "\n",
       "   DiabetesPedigreeFunction  Age  Outcome  \n",
       "0                     0.627   50        1  \n",
       "1                     0.351   31        0  \n",
       "2                     0.672   32        1  \n",
       "3                     0.167   21        0  \n",
       "4                     2.288   33        1  "
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('diabetes.csv'); # Pour tester notre modèle nous avons les données de prédiction du diabtète\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "06dbcdb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Séparation des Input et Output\n",
    "Y = df['Outcome']\n",
    "X = df.drop('Outcome', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "150c7aaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras.engine.sequential.Sequential object at 0x0000020347230DF0>\n"
     ]
    }
   ],
   "source": [
    "#initialisation du model\n",
    "model =tf.keras.models.Sequential()\n",
    "print(model)\n",
    "# 1.  input layer \n",
    "model.add(tf.keras.layers.Dense(8,input_dim=8,activation='relu'))\n",
    "# 2: Couche cachers\n",
    "model.add(tf.keras.layers.Dense(3,activation='sigmoid'))\n",
    "model.add(tf.keras.layers.Dense(5,activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(4,activation='sigmoid'))\n",
    "\n",
    "# 3 / output layers\n",
    "model.add(tf.keras.layers.Dense(2,activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "645ea310",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Visualisation du modèle\n",
    "from keras.utils.vis_utils import plot_model\n",
    "plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "69ff3dcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_12\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_60 (Dense)            (None, 8)                 72        \n",
      "                                                                 \n",
      " dense_61 (Dense)            (None, 3)                 27        \n",
      "                                                                 \n",
      " dense_62 (Dense)            (None, 5)                 20        \n",
      "                                                                 \n",
      " dense_63 (Dense)            (None, 4)                 24        \n",
      "                                                                 \n",
      " dense_64 (Dense)            (None, 2)                 10        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 153\n",
      "Trainable params: 153\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Voir la configuration du model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "49c05cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compilation du modèle\n",
    "model.compile(optimizer=tf.keras.optimizers.Adagrad(),loss='binary_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "40b64a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Séparation des données teste et train\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,Y_train,Y_test = train_test_split(X,Y, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "9cce4a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Centrer les données de sortie en deux catégories\n",
    "from keras.utils import np_utils\n",
    "y_train = np_utils.to_categorical(Y_train,2)\n",
    "y_test = np_utils.to_categorical(Y_test,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "91a63606",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "17/17 [==============================] - 1s 11ms/step - loss: 0.5932 - accuracy: 0.6834 - val_loss: 0.6115 - val_accuracy: 0.6450\n",
      "Epoch 2/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.5878 - accuracy: 0.6853 - val_loss: 0.6106 - val_accuracy: 0.6407\n",
      "Epoch 3/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.5873 - accuracy: 0.6909 - val_loss: 0.6103 - val_accuracy: 0.6364\n",
      "Epoch 4/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.5871 - accuracy: 0.6909 - val_loss: 0.6099 - val_accuracy: 0.6364\n",
      "Epoch 5/50\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 0.5867 - accuracy: 0.6890 - val_loss: 0.6096 - val_accuracy: 0.6364\n",
      "Epoch 6/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.5869 - accuracy: 0.6872 - val_loss: 0.6094 - val_accuracy: 0.6364\n",
      "Epoch 7/50\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.5865 - accuracy: 0.6853 - val_loss: 0.6093 - val_accuracy: 0.6320\n",
      "Epoch 8/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.5863 - accuracy: 0.6853 - val_loss: 0.6093 - val_accuracy: 0.6320\n",
      "Epoch 9/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.5862 - accuracy: 0.6872 - val_loss: 0.6093 - val_accuracy: 0.6364\n",
      "Epoch 10/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.5859 - accuracy: 0.6834 - val_loss: 0.6092 - val_accuracy: 0.6364\n",
      "Epoch 11/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.5858 - accuracy: 0.6890 - val_loss: 0.6091 - val_accuracy: 0.6364\n",
      "Epoch 12/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.5857 - accuracy: 0.6872 - val_loss: 0.6091 - val_accuracy: 0.6364\n",
      "Epoch 13/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.5856 - accuracy: 0.6872 - val_loss: 0.6090 - val_accuracy: 0.6364\n",
      "Epoch 14/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.5856 - accuracy: 0.6890 - val_loss: 0.6089 - val_accuracy: 0.6364\n",
      "Epoch 15/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.5854 - accuracy: 0.6927 - val_loss: 0.6089 - val_accuracy: 0.6320\n",
      "Epoch 16/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.5854 - accuracy: 0.6946 - val_loss: 0.6089 - val_accuracy: 0.6320\n",
      "Epoch 17/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.5852 - accuracy: 0.6946 - val_loss: 0.6089 - val_accuracy: 0.6320\n",
      "Epoch 18/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.5852 - accuracy: 0.6927 - val_loss: 0.6089 - val_accuracy: 0.6320\n",
      "Epoch 19/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.5851 - accuracy: 0.6927 - val_loss: 0.6088 - val_accuracy: 0.6320\n",
      "Epoch 20/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.5851 - accuracy: 0.6909 - val_loss: 0.6088 - val_accuracy: 0.6320\n",
      "Epoch 21/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.5852 - accuracy: 0.6909 - val_loss: 0.6088 - val_accuracy: 0.6320\n",
      "Epoch 22/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.5849 - accuracy: 0.6909 - val_loss: 0.6088 - val_accuracy: 0.6320\n",
      "Epoch 23/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.5847 - accuracy: 0.6909 - val_loss: 0.6087 - val_accuracy: 0.6320\n",
      "Epoch 24/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.5848 - accuracy: 0.6909 - val_loss: 0.6087 - val_accuracy: 0.6364\n",
      "Epoch 25/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.5847 - accuracy: 0.6890 - val_loss: 0.6087 - val_accuracy: 0.6364\n",
      "Epoch 26/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.5846 - accuracy: 0.6890 - val_loss: 0.6087 - val_accuracy: 0.6364\n",
      "Epoch 27/50\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.5846 - accuracy: 0.6909 - val_loss: 0.6086 - val_accuracy: 0.6407\n",
      "Epoch 28/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.5845 - accuracy: 0.6909 - val_loss: 0.6086 - val_accuracy: 0.6450\n",
      "Epoch 29/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.5844 - accuracy: 0.6909 - val_loss: 0.6086 - val_accuracy: 0.6450\n",
      "Epoch 30/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.5844 - accuracy: 0.6946 - val_loss: 0.6086 - val_accuracy: 0.6450\n",
      "Epoch 31/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.5843 - accuracy: 0.6927 - val_loss: 0.6086 - val_accuracy: 0.6450\n",
      "Epoch 32/50\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.5842 - accuracy: 0.6927 - val_loss: 0.6085 - val_accuracy: 0.6450\n",
      "Epoch 33/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.5842 - accuracy: 0.6909 - val_loss: 0.6085 - val_accuracy: 0.6450\n",
      "Epoch 34/50\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.5842 - accuracy: 0.6927 - val_loss: 0.6085 - val_accuracy: 0.6450\n",
      "Epoch 35/50\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.5841 - accuracy: 0.6909 - val_loss: 0.6085 - val_accuracy: 0.6450\n",
      "Epoch 36/50\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.5841 - accuracy: 0.6909 - val_loss: 0.6085 - val_accuracy: 0.6450\n",
      "Epoch 37/50\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.5841 - accuracy: 0.6890 - val_loss: 0.6085 - val_accuracy: 0.6450\n",
      "Epoch 38/50\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.5841 - accuracy: 0.6909 - val_loss: 0.6085 - val_accuracy: 0.6494\n",
      "Epoch 39/50\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.5840 - accuracy: 0.6890 - val_loss: 0.6084 - val_accuracy: 0.6494\n",
      "Epoch 40/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.5840 - accuracy: 0.6890 - val_loss: 0.6084 - val_accuracy: 0.6494\n",
      "Epoch 41/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.5840 - accuracy: 0.6890 - val_loss: 0.6084 - val_accuracy: 0.6494\n",
      "Epoch 42/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.5840 - accuracy: 0.6890 - val_loss: 0.6084 - val_accuracy: 0.6494\n",
      "Epoch 43/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.5840 - accuracy: 0.6909 - val_loss: 0.6084 - val_accuracy: 0.6494\n",
      "Epoch 44/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.5840 - accuracy: 0.6909 - val_loss: 0.6084 - val_accuracy: 0.6494\n",
      "Epoch 45/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.5839 - accuracy: 0.6909 - val_loss: 0.6084 - val_accuracy: 0.6494\n",
      "Epoch 46/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.5838 - accuracy: 0.6909 - val_loss: 0.6084 - val_accuracy: 0.6494\n",
      "Epoch 47/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.5838 - accuracy: 0.6909 - val_loss: 0.6084 - val_accuracy: 0.6494\n",
      "Epoch 48/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.5838 - accuracy: 0.6909 - val_loss: 0.6084 - val_accuracy: 0.6494\n",
      "Epoch 49/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.5838 - accuracy: 0.6927 - val_loss: 0.6084 - val_accuracy: 0.6494\n",
      "Epoch 50/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.5838 - accuracy: 0.6965 - val_loss: 0.6083 - val_accuracy: 0.6494\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x20348564d30>"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Entraînement du modèle\n",
    "model.fit(x=X_train, y=y_train, validation_data=(X_test, y_test), epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "07da1042",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 2ms/step\n",
      "accuracy du MLP 0.6406926406926406\n"
     ]
    }
   ],
   "source": [
    "pred_mlp = model.predict(X_test)\n",
    "mlp_acc = accuracy_score(y_test, pred_mlp.round())\n",
    "print('accuracy du MLP', mlp_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "a3b6c0e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "17/17 [==============================] - 1s 12ms/step - loss: 0.7603 - accuracy: 0.6387 - val_loss: 0.7446 - val_accuracy: 0.6797\n",
      "Epoch 2/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.7424 - accuracy: 0.6387 - val_loss: 0.7293 - val_accuracy: 0.6797\n",
      "Epoch 3/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.7301 - accuracy: 0.6387 - val_loss: 0.7180 - val_accuracy: 0.6797\n",
      "Epoch 4/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.7207 - accuracy: 0.6387 - val_loss: 0.7091 - val_accuracy: 0.6797\n",
      "Epoch 5/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.7132 - accuracy: 0.6387 - val_loss: 0.7017 - val_accuracy: 0.6797\n",
      "Epoch 6/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.7070 - accuracy: 0.6387 - val_loss: 0.6955 - val_accuracy: 0.6797\n",
      "Epoch 7/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.7017 - accuracy: 0.6387 - val_loss: 0.6902 - val_accuracy: 0.6797\n",
      "Epoch 8/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6972 - accuracy: 0.6387 - val_loss: 0.6856 - val_accuracy: 0.6797\n",
      "Epoch 9/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6933 - accuracy: 0.6387 - val_loss: 0.6816 - val_accuracy: 0.6797\n",
      "Epoch 10/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6899 - accuracy: 0.6387 - val_loss: 0.6780 - val_accuracy: 0.6797\n",
      "Epoch 11/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6869 - accuracy: 0.6387 - val_loss: 0.6748 - val_accuracy: 0.6797\n",
      "Epoch 12/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6842 - accuracy: 0.6387 - val_loss: 0.6719 - val_accuracy: 0.6797\n",
      "Epoch 13/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6818 - accuracy: 0.6387 - val_loss: 0.6693 - val_accuracy: 0.6797\n",
      "Epoch 14/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6797 - accuracy: 0.6387 - val_loss: 0.6670 - val_accuracy: 0.6797\n",
      "Epoch 15/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6778 - accuracy: 0.6387 - val_loss: 0.6648 - val_accuracy: 0.6797\n",
      "Epoch 16/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6760 - accuracy: 0.6387 - val_loss: 0.6629 - val_accuracy: 0.6797\n",
      "Epoch 17/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6744 - accuracy: 0.6387 - val_loss: 0.6610 - val_accuracy: 0.6797\n",
      "Epoch 18/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6730 - accuracy: 0.6387 - val_loss: 0.6593 - val_accuracy: 0.6797\n",
      "Epoch 19/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6716 - accuracy: 0.6387 - val_loss: 0.6578 - val_accuracy: 0.6797\n",
      "Epoch 20/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6704 - accuracy: 0.6387 - val_loss: 0.6563 - val_accuracy: 0.6797\n",
      "Epoch 21/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6693 - accuracy: 0.6387 - val_loss: 0.6550 - val_accuracy: 0.6797\n",
      "Epoch 22/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6683 - accuracy: 0.6387 - val_loss: 0.6537 - val_accuracy: 0.6797\n",
      "Epoch 23/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6674 - accuracy: 0.6387 - val_loss: 0.6526 - val_accuracy: 0.6797\n",
      "Epoch 24/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6665 - accuracy: 0.6387 - val_loss: 0.6516 - val_accuracy: 0.6797\n",
      "Epoch 25/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6657 - accuracy: 0.6387 - val_loss: 0.6505 - val_accuracy: 0.6797\n",
      "Epoch 26/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6649 - accuracy: 0.6387 - val_loss: 0.6496 - val_accuracy: 0.6797\n",
      "Epoch 27/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6643 - accuracy: 0.6387 - val_loss: 0.6488 - val_accuracy: 0.6797\n",
      "Epoch 28/50\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.6636 - accuracy: 0.6387 - val_loss: 0.6480 - val_accuracy: 0.6797\n",
      "Epoch 29/50\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.6630 - accuracy: 0.6387 - val_loss: 0.6471 - val_accuracy: 0.6797\n",
      "Epoch 30/50\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.6625 - accuracy: 0.6387 - val_loss: 0.6464 - val_accuracy: 0.6797\n",
      "Epoch 31/50\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 0.6620 - accuracy: 0.6387 - val_loss: 0.6457 - val_accuracy: 0.6797\n",
      "Epoch 32/50\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.6615 - accuracy: 0.6387 - val_loss: 0.6451 - val_accuracy: 0.6797\n",
      "Epoch 33/50\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.6610 - accuracy: 0.6387 - val_loss: 0.6445 - val_accuracy: 0.6797\n",
      "Epoch 34/50\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 0.6606 - accuracy: 0.6387 - val_loss: 0.6438 - val_accuracy: 0.6797\n",
      "Epoch 35/50\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.6602 - accuracy: 0.6387 - val_loss: 0.6433 - val_accuracy: 0.6797\n",
      "Epoch 36/50\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.6598 - accuracy: 0.6387 - val_loss: 0.6428 - val_accuracy: 0.6797\n",
      "Epoch 37/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6595 - accuracy: 0.6387 - val_loss: 0.6424 - val_accuracy: 0.6797\n",
      "Epoch 38/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6592 - accuracy: 0.6387 - val_loss: 0.6419 - val_accuracy: 0.6797\n",
      "Epoch 39/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6589 - accuracy: 0.6387 - val_loss: 0.6414 - val_accuracy: 0.6797\n",
      "Epoch 40/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6587 - accuracy: 0.6387 - val_loss: 0.6409 - val_accuracy: 0.6797\n",
      "Epoch 41/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6584 - accuracy: 0.6387 - val_loss: 0.6406 - val_accuracy: 0.6797\n",
      "Epoch 42/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6581 - accuracy: 0.6387 - val_loss: 0.6402 - val_accuracy: 0.6797\n",
      "Epoch 43/50\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 0.6579 - accuracy: 0.6387 - val_loss: 0.6399 - val_accuracy: 0.6797\n",
      "Epoch 44/50\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 0.6577 - accuracy: 0.6387 - val_loss: 0.6395 - val_accuracy: 0.6797\n",
      "Epoch 45/50\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.6575 - accuracy: 0.6387 - val_loss: 0.6393 - val_accuracy: 0.6797\n",
      "Epoch 46/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6573 - accuracy: 0.6387 - val_loss: 0.6389 - val_accuracy: 0.6797\n",
      "Epoch 47/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6571 - accuracy: 0.6387 - val_loss: 0.6386 - val_accuracy: 0.6797\n",
      "Epoch 48/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6570 - accuracy: 0.6387 - val_loss: 0.6384 - val_accuracy: 0.6797\n",
      "Epoch 49/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6568 - accuracy: 0.6387 - val_loss: 0.6380 - val_accuracy: 0.6797\n",
      "Epoch 50/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6567 - accuracy: 0.6387 - val_loss: 0.6378 - val_accuracy: 0.6797\n",
      "8/8 [==============================] - 0s 1ms/step\n",
      "Epoch 1/50\n",
      "17/17 [==============================] - 1s 13ms/step - loss: 0.6564 - accuracy: 0.6387 - val_loss: 0.6359 - val_accuracy: 0.6797\n",
      "Epoch 2/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6555 - accuracy: 0.6387 - val_loss: 0.6348 - val_accuracy: 0.6797\n",
      "Epoch 3/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6551 - accuracy: 0.6387 - val_loss: 0.6339 - val_accuracy: 0.6797\n",
      "Epoch 4/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6548 - accuracy: 0.6387 - val_loss: 0.6332 - val_accuracy: 0.6797\n",
      "Epoch 5/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6546 - accuracy: 0.6387 - val_loss: 0.6327 - val_accuracy: 0.6797\n",
      "Epoch 6/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6546 - accuracy: 0.6387 - val_loss: 0.6324 - val_accuracy: 0.6797\n",
      "Epoch 7/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6543 - accuracy: 0.6387 - val_loss: 0.6323 - val_accuracy: 0.6797\n",
      "Epoch 8/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6541 - accuracy: 0.6387 - val_loss: 0.6318 - val_accuracy: 0.6797\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6539 - accuracy: 0.6387 - val_loss: 0.6317 - val_accuracy: 0.6797\n",
      "Epoch 10/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6539 - accuracy: 0.6387 - val_loss: 0.6318 - val_accuracy: 0.6797\n",
      "Epoch 11/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6537 - accuracy: 0.6387 - val_loss: 0.6315 - val_accuracy: 0.6797\n",
      "Epoch 12/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6536 - accuracy: 0.6387 - val_loss: 0.6315 - val_accuracy: 0.6797\n",
      "Epoch 13/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6533 - accuracy: 0.6387 - val_loss: 0.6313 - val_accuracy: 0.6797\n",
      "Epoch 14/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6534 - accuracy: 0.6387 - val_loss: 0.6312 - val_accuracy: 0.6797\n",
      "Epoch 15/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6532 - accuracy: 0.6387 - val_loss: 0.6310 - val_accuracy: 0.6797\n",
      "Epoch 16/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6532 - accuracy: 0.6387 - val_loss: 0.6308 - val_accuracy: 0.6797\n",
      "Epoch 17/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6530 - accuracy: 0.6387 - val_loss: 0.6305 - val_accuracy: 0.6797\n",
      "Epoch 18/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6530 - accuracy: 0.6387 - val_loss: 0.6304 - val_accuracy: 0.6797\n",
      "Epoch 19/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6529 - accuracy: 0.6387 - val_loss: 0.6302 - val_accuracy: 0.6797\n",
      "Epoch 20/50\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.6528 - accuracy: 0.6387 - val_loss: 0.6301 - val_accuracy: 0.6797\n",
      "Epoch 21/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6527 - accuracy: 0.6387 - val_loss: 0.6300 - val_accuracy: 0.6797\n",
      "Epoch 22/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6525 - accuracy: 0.6387 - val_loss: 0.6298 - val_accuracy: 0.6797\n",
      "Epoch 23/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6524 - accuracy: 0.6387 - val_loss: 0.6297 - val_accuracy: 0.6797\n",
      "Epoch 24/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6526 - accuracy: 0.6387 - val_loss: 0.6296 - val_accuracy: 0.6797\n",
      "Epoch 25/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6523 - accuracy: 0.6387 - val_loss: 0.6295 - val_accuracy: 0.6797\n",
      "Epoch 26/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6523 - accuracy: 0.6387 - val_loss: 0.6295 - val_accuracy: 0.6797\n",
      "Epoch 27/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6523 - accuracy: 0.6387 - val_loss: 0.6294 - val_accuracy: 0.6797\n",
      "Epoch 28/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6521 - accuracy: 0.6387 - val_loss: 0.6293 - val_accuracy: 0.6797\n",
      "Epoch 29/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6522 - accuracy: 0.6387 - val_loss: 0.6292 - val_accuracy: 0.6797\n",
      "Epoch 30/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6521 - accuracy: 0.6387 - val_loss: 0.6291 - val_accuracy: 0.6797\n",
      "Epoch 31/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6520 - accuracy: 0.6387 - val_loss: 0.6292 - val_accuracy: 0.6797\n",
      "Epoch 32/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6521 - accuracy: 0.6387 - val_loss: 0.6291 - val_accuracy: 0.6797\n",
      "Epoch 33/50\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.6521 - accuracy: 0.6387 - val_loss: 0.6290 - val_accuracy: 0.6797\n",
      "Epoch 34/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6518 - accuracy: 0.6387 - val_loss: 0.6289 - val_accuracy: 0.6797\n",
      "Epoch 35/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6519 - accuracy: 0.6387 - val_loss: 0.6288 - val_accuracy: 0.6797\n",
      "Epoch 36/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6516 - accuracy: 0.6387 - val_loss: 0.6287 - val_accuracy: 0.6797\n",
      "Epoch 37/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6516 - accuracy: 0.6387 - val_loss: 0.6286 - val_accuracy: 0.6797\n",
      "Epoch 38/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6515 - accuracy: 0.6387 - val_loss: 0.6286 - val_accuracy: 0.6797\n",
      "Epoch 39/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6515 - accuracy: 0.6387 - val_loss: 0.6285 - val_accuracy: 0.6797\n",
      "Epoch 40/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6514 - accuracy: 0.6387 - val_loss: 0.6285 - val_accuracy: 0.6797\n",
      "Epoch 41/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6513 - accuracy: 0.6387 - val_loss: 0.6284 - val_accuracy: 0.6797\n",
      "Epoch 42/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6511 - accuracy: 0.6387 - val_loss: 0.6287 - val_accuracy: 0.6797\n",
      "Epoch 43/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6514 - accuracy: 0.6387 - val_loss: 0.6285 - val_accuracy: 0.6797\n",
      "Epoch 44/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6512 - accuracy: 0.6387 - val_loss: 0.6283 - val_accuracy: 0.6797\n",
      "Epoch 45/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6511 - accuracy: 0.6387 - val_loss: 0.6282 - val_accuracy: 0.6797\n",
      "Epoch 46/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6507 - accuracy: 0.6387 - val_loss: 0.6282 - val_accuracy: 0.6797\n",
      "Epoch 47/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6510 - accuracy: 0.6387 - val_loss: 0.6283 - val_accuracy: 0.6797\n",
      "Epoch 48/50\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.6508 - accuracy: 0.6387 - val_loss: 0.6282 - val_accuracy: 0.6797\n",
      "Epoch 49/50\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.6509 - accuracy: 0.6387 - val_loss: 0.6282 - val_accuracy: 0.6797\n",
      "Epoch 50/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6506 - accuracy: 0.6387 - val_loss: 0.6281 - val_accuracy: 0.6797\n",
      "8/8 [==============================] - 0s 1ms/step\n",
      "Epoch 1/50\n",
      "17/17 [==============================] - 1s 42ms/step - loss: 0.6513 - accuracy: 0.6387 - val_loss: 0.6297 - val_accuracy: 0.6797\n",
      "Epoch 2/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6509 - accuracy: 0.6387 - val_loss: 0.6284 - val_accuracy: 0.6797\n",
      "Epoch 3/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6516 - accuracy: 0.6387 - val_loss: 0.6294 - val_accuracy: 0.6797\n",
      "Epoch 4/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6532 - accuracy: 0.6387 - val_loss: 0.6281 - val_accuracy: 0.6797\n",
      "Epoch 5/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6508 - accuracy: 0.6387 - val_loss: 0.6283 - val_accuracy: 0.6797\n",
      "Epoch 6/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6516 - accuracy: 0.6387 - val_loss: 0.6285 - val_accuracy: 0.6797\n",
      "Epoch 7/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6509 - accuracy: 0.6387 - val_loss: 0.6277 - val_accuracy: 0.6797\n",
      "Epoch 8/50\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 0.6507 - accuracy: 0.6387 - val_loss: 0.6276 - val_accuracy: 0.6797\n",
      "Epoch 9/50\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.6498 - accuracy: 0.6387 - val_loss: 0.6273 - val_accuracy: 0.6797\n",
      "Epoch 10/50\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.6501 - accuracy: 0.6387 - val_loss: 0.6271 - val_accuracy: 0.6797\n",
      "Epoch 11/50\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.6501 - accuracy: 0.6387 - val_loss: 0.6269 - val_accuracy: 0.6797\n",
      "Epoch 12/50\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.6502 - accuracy: 0.6387 - val_loss: 0.6270 - val_accuracy: 0.6797\n",
      "Epoch 13/50\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.6501 - accuracy: 0.6387 - val_loss: 0.6271 - val_accuracy: 0.6797\n",
      "Epoch 14/50\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 0.6500 - accuracy: 0.6387 - val_loss: 0.6269 - val_accuracy: 0.6797\n",
      "Epoch 15/50\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.6491 - accuracy: 0.6387 - val_loss: 0.6267 - val_accuracy: 0.6797\n",
      "Epoch 16/50\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.6501 - accuracy: 0.6387 - val_loss: 0.6269 - val_accuracy: 0.6797\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6495 - accuracy: 0.6387 - val_loss: 0.6265 - val_accuracy: 0.6797\n",
      "Epoch 18/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6492 - accuracy: 0.6387 - val_loss: 0.6264 - val_accuracy: 0.6797\n",
      "Epoch 19/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6495 - accuracy: 0.6387 - val_loss: 0.6264 - val_accuracy: 0.6797\n",
      "Epoch 20/50\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 0.6494 - accuracy: 0.6387 - val_loss: 0.6263 - val_accuracy: 0.6797\n",
      "Epoch 21/50\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.6488 - accuracy: 0.6387 - val_loss: 0.6263 - val_accuracy: 0.6797\n",
      "Epoch 22/50\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.6489 - accuracy: 0.6387 - val_loss: 0.6261 - val_accuracy: 0.6797\n",
      "Epoch 23/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6482 - accuracy: 0.6387 - val_loss: 0.6260 - val_accuracy: 0.6797\n",
      "Epoch 24/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6492 - accuracy: 0.6387 - val_loss: 0.6260 - val_accuracy: 0.6797\n",
      "Epoch 25/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6488 - accuracy: 0.6387 - val_loss: 0.6258 - val_accuracy: 0.6797\n",
      "Epoch 26/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6476 - accuracy: 0.6387 - val_loss: 0.6260 - val_accuracy: 0.6797\n",
      "Epoch 27/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6487 - accuracy: 0.6387 - val_loss: 0.6283 - val_accuracy: 0.6797\n",
      "Epoch 28/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6483 - accuracy: 0.6387 - val_loss: 0.6251 - val_accuracy: 0.6797\n",
      "Epoch 29/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6477 - accuracy: 0.6387 - val_loss: 0.6256 - val_accuracy: 0.6797\n",
      "Epoch 30/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6479 - accuracy: 0.6387 - val_loss: 0.6253 - val_accuracy: 0.6797\n",
      "Epoch 31/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6481 - accuracy: 0.6387 - val_loss: 0.6251 - val_accuracy: 0.6797\n",
      "Epoch 32/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6478 - accuracy: 0.6387 - val_loss: 0.6247 - val_accuracy: 0.6797\n",
      "Epoch 33/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6472 - accuracy: 0.6387 - val_loss: 0.6250 - val_accuracy: 0.6797\n",
      "Epoch 34/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6464 - accuracy: 0.6387 - val_loss: 0.6253 - val_accuracy: 0.6797\n",
      "Epoch 35/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6462 - accuracy: 0.6387 - val_loss: 0.6252 - val_accuracy: 0.6797\n",
      "Epoch 36/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6471 - accuracy: 0.6387 - val_loss: 0.6245 - val_accuracy: 0.6797\n",
      "Epoch 37/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6470 - accuracy: 0.6387 - val_loss: 0.6243 - val_accuracy: 0.6797\n",
      "Epoch 38/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6461 - accuracy: 0.6387 - val_loss: 0.6284 - val_accuracy: 0.6797\n",
      "Epoch 39/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6482 - accuracy: 0.6387 - val_loss: 0.6246 - val_accuracy: 0.6797\n",
      "Epoch 40/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6465 - accuracy: 0.6387 - val_loss: 0.6239 - val_accuracy: 0.6797\n",
      "Epoch 41/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6465 - accuracy: 0.6387 - val_loss: 0.6240 - val_accuracy: 0.6797\n",
      "Epoch 42/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6476 - accuracy: 0.6387 - val_loss: 0.6240 - val_accuracy: 0.6797\n",
      "Epoch 43/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6453 - accuracy: 0.6387 - val_loss: 0.6240 - val_accuracy: 0.6797\n",
      "Epoch 44/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6453 - accuracy: 0.6387 - val_loss: 0.6241 - val_accuracy: 0.6797\n",
      "Epoch 45/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6457 - accuracy: 0.6387 - val_loss: 0.6241 - val_accuracy: 0.6797\n",
      "Epoch 46/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6457 - accuracy: 0.6387 - val_loss: 0.6237 - val_accuracy: 0.6797\n",
      "Epoch 47/50\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.6458 - accuracy: 0.6387 - val_loss: 0.6237 - val_accuracy: 0.6797\n",
      "Epoch 48/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6450 - accuracy: 0.6387 - val_loss: 0.6234 - val_accuracy: 0.6797\n",
      "Epoch 49/50\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.6449 - accuracy: 0.6387 - val_loss: 0.6235 - val_accuracy: 0.6797\n",
      "Epoch 50/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6444 - accuracy: 0.6387 - val_loss: 0.6236 - val_accuracy: 0.6797\n",
      "8/8 [==============================] - 0s 1ms/step\n",
      "Epoch 1/50\n",
      "17/17 [==============================] - 1s 12ms/step - loss: 0.6502 - accuracy: 0.6387 - val_loss: 0.6229 - val_accuracy: 0.6797\n",
      "Epoch 2/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6474 - accuracy: 0.6387 - val_loss: 0.6237 - val_accuracy: 0.6797\n",
      "Epoch 3/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6508 - accuracy: 0.6387 - val_loss: 0.6261 - val_accuracy: 0.6797\n",
      "Epoch 4/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6463 - accuracy: 0.6387 - val_loss: 0.6244 - val_accuracy: 0.6797\n",
      "Epoch 5/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6469 - accuracy: 0.6387 - val_loss: 0.6240 - val_accuracy: 0.6797\n",
      "Epoch 6/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6473 - accuracy: 0.6387 - val_loss: 0.6251 - val_accuracy: 0.6797\n",
      "Epoch 7/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6487 - accuracy: 0.6387 - val_loss: 0.6247 - val_accuracy: 0.6797\n",
      "Epoch 8/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6461 - accuracy: 0.6387 - val_loss: 0.6267 - val_accuracy: 0.6797\n",
      "Epoch 9/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6486 - accuracy: 0.6387 - val_loss: 0.6242 - val_accuracy: 0.6797\n",
      "Epoch 10/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6460 - accuracy: 0.6387 - val_loss: 0.6220 - val_accuracy: 0.6797\n",
      "Epoch 11/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6449 - accuracy: 0.6387 - val_loss: 0.6237 - val_accuracy: 0.6797\n",
      "Epoch 12/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6458 - accuracy: 0.6387 - val_loss: 0.6277 - val_accuracy: 0.6797\n",
      "Epoch 13/50\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.6514 - accuracy: 0.6387 - val_loss: 0.6252 - val_accuracy: 0.6797\n",
      "Epoch 14/50\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.6457 - accuracy: 0.6387 - val_loss: 0.6232 - val_accuracy: 0.6797\n",
      "Epoch 15/50\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.6452 - accuracy: 0.6387 - val_loss: 0.6227 - val_accuracy: 0.6797\n",
      "Epoch 16/50\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.6484 - accuracy: 0.6387 - val_loss: 0.6255 - val_accuracy: 0.6797\n",
      "Epoch 17/50\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.6451 - accuracy: 0.6387 - val_loss: 0.6239 - val_accuracy: 0.6797\n",
      "Epoch 18/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6431 - accuracy: 0.6387 - val_loss: 0.6233 - val_accuracy: 0.6797\n",
      "Epoch 19/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6418 - accuracy: 0.6387 - val_loss: 0.6219 - val_accuracy: 0.6797\n",
      "Epoch 20/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6411 - accuracy: 0.6387 - val_loss: 0.6221 - val_accuracy: 0.6797\n",
      "Epoch 21/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6420 - accuracy: 0.6387 - val_loss: 0.6231 - val_accuracy: 0.6797\n",
      "Epoch 22/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6449 - accuracy: 0.6387 - val_loss: 0.6221 - val_accuracy: 0.6797\n",
      "Epoch 23/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6436 - accuracy: 0.6387 - val_loss: 0.6217 - val_accuracy: 0.6797\n",
      "Epoch 24/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6419 - accuracy: 0.6387 - val_loss: 0.6218 - val_accuracy: 0.6797\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6432 - accuracy: 0.6387 - val_loss: 0.6230 - val_accuracy: 0.6797\n",
      "Epoch 26/50\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.6429 - accuracy: 0.6387 - val_loss: 0.6228 - val_accuracy: 0.6797\n",
      "Epoch 27/50\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.6417 - accuracy: 0.6387 - val_loss: 0.6210 - val_accuracy: 0.6797\n",
      "Epoch 28/50\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.6411 - accuracy: 0.6387 - val_loss: 0.6209 - val_accuracy: 0.6797\n",
      "Epoch 29/50\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.6433 - accuracy: 0.6387 - val_loss: 0.6288 - val_accuracy: 0.6797\n",
      "Epoch 30/50\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.6433 - accuracy: 0.6387 - val_loss: 0.6212 - val_accuracy: 0.6797\n",
      "Epoch 31/50\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.6409 - accuracy: 0.6387 - val_loss: 0.6218 - val_accuracy: 0.6797\n",
      "Epoch 32/50\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 0.6413 - accuracy: 0.6387 - val_loss: 0.6215 - val_accuracy: 0.6797\n",
      "Epoch 33/50\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.6435 - accuracy: 0.6387 - val_loss: 0.6234 - val_accuracy: 0.6797\n",
      "Epoch 34/50\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.6398 - accuracy: 0.6387 - val_loss: 0.6208 - val_accuracy: 0.6797\n",
      "Epoch 35/50\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 0.6378 - accuracy: 0.6387 - val_loss: 0.6224 - val_accuracy: 0.6797\n",
      "Epoch 36/50\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.6427 - accuracy: 0.6387 - val_loss: 0.6221 - val_accuracy: 0.6797\n",
      "Epoch 37/50\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 0.6402 - accuracy: 0.6387 - val_loss: 0.6230 - val_accuracy: 0.6797\n",
      "Epoch 38/50\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.6390 - accuracy: 0.6387 - val_loss: 0.6207 - val_accuracy: 0.6797\n",
      "Epoch 39/50\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.6381 - accuracy: 0.6387 - val_loss: 0.6210 - val_accuracy: 0.6797\n",
      "Epoch 40/50\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.6415 - accuracy: 0.6387 - val_loss: 0.6209 - val_accuracy: 0.6797\n",
      "Epoch 41/50\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 0.6391 - accuracy: 0.6387 - val_loss: 0.6223 - val_accuracy: 0.6797\n",
      "Epoch 42/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6375 - accuracy: 0.6387 - val_loss: 0.6227 - val_accuracy: 0.6797\n",
      "Epoch 43/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6386 - accuracy: 0.6387 - val_loss: 0.6227 - val_accuracy: 0.6797\n",
      "Epoch 44/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6371 - accuracy: 0.6387 - val_loss: 0.6188 - val_accuracy: 0.6797\n",
      "Epoch 45/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6390 - accuracy: 0.6387 - val_loss: 0.6211 - val_accuracy: 0.6797\n",
      "Epoch 46/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6405 - accuracy: 0.6387 - val_loss: 0.6185 - val_accuracy: 0.6797\n",
      "Epoch 47/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6409 - accuracy: 0.6387 - val_loss: 0.6186 - val_accuracy: 0.6797\n",
      "Epoch 48/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6374 - accuracy: 0.6387 - val_loss: 0.6215 - val_accuracy: 0.6797\n",
      "Epoch 49/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6385 - accuracy: 0.6387 - val_loss: 0.6206 - val_accuracy: 0.6797\n",
      "Epoch 50/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6375 - accuracy: 0.6387 - val_loss: 0.6208 - val_accuracy: 0.6797\n",
      "8/8 [==============================] - 0s 2ms/step\n",
      "Epoch 1/50\n",
      "17/17 [==============================] - 1s 11ms/step - loss: 0.6550 - accuracy: 0.6387 - val_loss: 0.6197 - val_accuracy: 0.6797\n",
      "Epoch 2/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6453 - accuracy: 0.6387 - val_loss: 0.6226 - val_accuracy: 0.6797\n",
      "Epoch 3/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6470 - accuracy: 0.6387 - val_loss: 0.6192 - val_accuracy: 0.6797\n",
      "Epoch 4/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6417 - accuracy: 0.6387 - val_loss: 0.6187 - val_accuracy: 0.6797\n",
      "Epoch 5/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6431 - accuracy: 0.6387 - val_loss: 0.6225 - val_accuracy: 0.6797\n",
      "Epoch 6/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6446 - accuracy: 0.6387 - val_loss: 0.6180 - val_accuracy: 0.6797\n",
      "Epoch 7/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6449 - accuracy: 0.6387 - val_loss: 0.6224 - val_accuracy: 0.6797\n",
      "Epoch 8/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6475 - accuracy: 0.6387 - val_loss: 0.6144 - val_accuracy: 0.6797\n",
      "Epoch 9/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6441 - accuracy: 0.6387 - val_loss: 0.6152 - val_accuracy: 0.6797\n",
      "Epoch 10/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6438 - accuracy: 0.6387 - val_loss: 0.6135 - val_accuracy: 0.6797\n",
      "Epoch 11/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6397 - accuracy: 0.6387 - val_loss: 0.6106 - val_accuracy: 0.6797\n",
      "Epoch 12/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6446 - accuracy: 0.6387 - val_loss: 0.6145 - val_accuracy: 0.6797\n",
      "Epoch 13/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6405 - accuracy: 0.6387 - val_loss: 0.6129 - val_accuracy: 0.6797\n",
      "Epoch 14/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6400 - accuracy: 0.6387 - val_loss: 0.6133 - val_accuracy: 0.6797\n",
      "Epoch 15/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6394 - accuracy: 0.6387 - val_loss: 0.6189 - val_accuracy: 0.6797\n",
      "Epoch 16/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6348 - accuracy: 0.6387 - val_loss: 0.6121 - val_accuracy: 0.6797\n",
      "Epoch 17/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6387 - accuracy: 0.6387 - val_loss: 0.6116 - val_accuracy: 0.6797\n",
      "Epoch 18/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6406 - accuracy: 0.6387 - val_loss: 0.6187 - val_accuracy: 0.6797\n",
      "Epoch 19/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6380 - accuracy: 0.6387 - val_loss: 0.6174 - val_accuracy: 0.6797\n",
      "Epoch 20/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6520 - accuracy: 0.6387 - val_loss: 0.6190 - val_accuracy: 0.6797\n",
      "Epoch 21/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6358 - accuracy: 0.6387 - val_loss: 0.6054 - val_accuracy: 0.6797\n",
      "Epoch 22/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6235 - accuracy: 0.6387 - val_loss: 0.6076 - val_accuracy: 0.6797\n",
      "Epoch 23/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6283 - accuracy: 0.6387 - val_loss: 0.6044 - val_accuracy: 0.6797\n",
      "Epoch 24/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6151 - accuracy: 0.6387 - val_loss: 0.6040 - val_accuracy: 0.6797\n",
      "Epoch 25/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6134 - accuracy: 0.6387 - val_loss: 0.6031 - val_accuracy: 0.6797\n",
      "Epoch 26/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6109 - accuracy: 0.6387 - val_loss: 0.6067 - val_accuracy: 0.6797\n",
      "Epoch 27/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6102 - accuracy: 0.6387 - val_loss: 0.6059 - val_accuracy: 0.6797\n",
      "Epoch 28/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6093 - accuracy: 0.6387 - val_loss: 0.5980 - val_accuracy: 0.6797\n",
      "Epoch 29/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6039 - accuracy: 0.6387 - val_loss: 0.6022 - val_accuracy: 0.6797\n",
      "Epoch 30/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6046 - accuracy: 0.6387 - val_loss: 0.6068 - val_accuracy: 0.6797\n",
      "Epoch 31/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6042 - accuracy: 0.6499 - val_loss: 0.6096 - val_accuracy: 0.6797\n",
      "Epoch 32/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6006 - accuracy: 0.6816 - val_loss: 0.5954 - val_accuracy: 0.7273\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6149 - accuracy: 0.6574 - val_loss: 0.6074 - val_accuracy: 0.6667\n",
      "Epoch 34/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6005 - accuracy: 0.6723 - val_loss: 0.6028 - val_accuracy: 0.6840\n",
      "Epoch 35/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6004 - accuracy: 0.6890 - val_loss: 0.6087 - val_accuracy: 0.6364\n",
      "Epoch 36/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.5986 - accuracy: 0.6816 - val_loss: 0.6019 - val_accuracy: 0.6710\n",
      "Epoch 37/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6051 - accuracy: 0.6797 - val_loss: 0.6088 - val_accuracy: 0.6580\n",
      "Epoch 38/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.5965 - accuracy: 0.6797 - val_loss: 0.5980 - val_accuracy: 0.6883\n",
      "Epoch 39/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6073 - accuracy: 0.6741 - val_loss: 0.6072 - val_accuracy: 0.6407\n",
      "Epoch 40/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6010 - accuracy: 0.6816 - val_loss: 0.6024 - val_accuracy: 0.6797\n",
      "Epoch 41/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6024 - accuracy: 0.6816 - val_loss: 0.6061 - val_accuracy: 0.6580\n",
      "Epoch 42/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.5901 - accuracy: 0.6872 - val_loss: 0.5875 - val_accuracy: 0.7013\n",
      "Epoch 43/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6006 - accuracy: 0.6760 - val_loss: 0.6060 - val_accuracy: 0.6537\n",
      "Epoch 44/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.5928 - accuracy: 0.6797 - val_loss: 0.6050 - val_accuracy: 0.6667\n",
      "Epoch 45/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6002 - accuracy: 0.6909 - val_loss: 0.6052 - val_accuracy: 0.6580\n",
      "Epoch 46/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.5968 - accuracy: 0.6946 - val_loss: 0.6055 - val_accuracy: 0.6494\n",
      "Epoch 47/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.5952 - accuracy: 0.6872 - val_loss: 0.6060 - val_accuracy: 0.6537\n",
      "Epoch 48/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.5969 - accuracy: 0.6890 - val_loss: 0.6025 - val_accuracy: 0.6623\n",
      "Epoch 49/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.5982 - accuracy: 0.6965 - val_loss: 0.6028 - val_accuracy: 0.6623\n",
      "Epoch 50/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.5920 - accuracy: 0.6909 - val_loss: 0.6177 - val_accuracy: 0.6320\n",
      "8/8 [==============================] - 0s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "# test du modèle avec des lr static\n",
    "lr=[0.01,0.02,0.03,0.04,0.05]\n",
    "acc2=[]\n",
    "for i in lr :\n",
    "    # Compilation du modèle\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adagrad(learning_rate=i),loss='binary_crossentropy',metrics=['accuracy'])\n",
    "    X_train,X_test,Y_train,Y_test = train_test_split(X,Y, test_size=0.3, random_state=0)\n",
    "    y_train = np_utils.to_categorical(Y_train,2)\n",
    "    y_test = np_utils.to_categorical(Y_test,2)\n",
    "    model.fit(x=X_train, y=y_train, validation_data=(X_test, y_test), epochs=50)\n",
    "    pred_mlp2 = model.predict(X_test)\n",
    "    acc2.append('Pour lr='+str(i)+' '+str(accuracy_score(y_test, pred_mlp2.round())))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "fb8a4be4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Pour lr=0.01 0.6796536796536796',\n",
       " 'Pour lr=0.02 0.6796536796536796',\n",
       " 'Pour lr=0.03 0.6796536796536796',\n",
       " 'Pour lr=0.04 0.5627705627705628',\n",
       " 'Pour lr=0.05 0.6277056277056277']"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0c72e9",
   "metadata": {},
   "source": [
    "Ce phénomène explique que pour Adagrade nous avons pas besoins d'optimiser le taux d'apprentisagge il ce charge automatiquement du taux d'apprentissage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89804c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import np_utils\n",
    "y_train = np_utils.to_categorical(Y_train,2)\n",
    "y_test = np_utils.to_categorical(Y_test,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc65b09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "ba08811e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "17/17 [==============================] - 1s 14ms/step - loss: 0.6312 - accuracy: 0.6387 - val_loss: 0.6090 - val_accuracy: 0.6797\n",
      "Epoch 2/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6298 - accuracy: 0.6387 - val_loss: 0.6085 - val_accuracy: 0.6797\n",
      "Epoch 3/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6291 - accuracy: 0.6387 - val_loss: 0.6075 - val_accuracy: 0.6797\n",
      "Epoch 4/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6284 - accuracy: 0.6387 - val_loss: 0.6076 - val_accuracy: 0.6797\n",
      "Epoch 5/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6291 - accuracy: 0.6387 - val_loss: 0.6066 - val_accuracy: 0.6797\n",
      "Epoch 6/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6267 - accuracy: 0.6387 - val_loss: 0.6056 - val_accuracy: 0.6797\n",
      "Epoch 7/50\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.6262 - accuracy: 0.6387 - val_loss: 0.6047 - val_accuracy: 0.6797\n",
      "Epoch 8/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6258 - accuracy: 0.6387 - val_loss: 0.6037 - val_accuracy: 0.6797\n",
      "Epoch 9/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6254 - accuracy: 0.6387 - val_loss: 0.6031 - val_accuracy: 0.6797\n",
      "Epoch 10/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6253 - accuracy: 0.6387 - val_loss: 0.6048 - val_accuracy: 0.6797\n",
      "Epoch 11/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6232 - accuracy: 0.6387 - val_loss: 0.6022 - val_accuracy: 0.6797\n",
      "Epoch 12/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6215 - accuracy: 0.6387 - val_loss: 0.6013 - val_accuracy: 0.6797\n",
      "Epoch 13/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6213 - accuracy: 0.6387 - val_loss: 0.5998 - val_accuracy: 0.6797\n",
      "Epoch 14/50\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.6198 - accuracy: 0.6387 - val_loss: 0.6000 - val_accuracy: 0.6797\n",
      "Epoch 15/50\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 0.6200 - accuracy: 0.6387 - val_loss: 0.5986 - val_accuracy: 0.6797\n",
      "Epoch 16/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6174 - accuracy: 0.6387 - val_loss: 0.5983 - val_accuracy: 0.6797\n",
      "Epoch 17/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6160 - accuracy: 0.6387 - val_loss: 0.5979 - val_accuracy: 0.6797\n",
      "Epoch 18/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6147 - accuracy: 0.6387 - val_loss: 0.5983 - val_accuracy: 0.6797\n",
      "Epoch 19/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6142 - accuracy: 0.6387 - val_loss: 0.5960 - val_accuracy: 0.6797\n",
      "Epoch 20/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6130 - accuracy: 0.6387 - val_loss: 0.5959 - val_accuracy: 0.6797\n",
      "Epoch 21/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6129 - accuracy: 0.6387 - val_loss: 0.5963 - val_accuracy: 0.6797\n",
      "Epoch 22/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6159 - accuracy: 0.6387 - val_loss: 0.5948 - val_accuracy: 0.6797\n",
      "Epoch 23/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6106 - accuracy: 0.6387 - val_loss: 0.5939 - val_accuracy: 0.6797\n",
      "Epoch 24/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6097 - accuracy: 0.6387 - val_loss: 0.5949 - val_accuracy: 0.6797\n",
      "Epoch 25/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6087 - accuracy: 0.6387 - val_loss: 0.5932 - val_accuracy: 0.6797\n",
      "Epoch 26/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6090 - accuracy: 0.6387 - val_loss: 0.5924 - val_accuracy: 0.6797\n",
      "Epoch 27/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6066 - accuracy: 0.6387 - val_loss: 0.5918 - val_accuracy: 0.6797\n",
      "Epoch 28/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6048 - accuracy: 0.6387 - val_loss: 0.5911 - val_accuracy: 0.6797\n",
      "Epoch 29/50\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.6039 - accuracy: 0.6387 - val_loss: 0.5913 - val_accuracy: 0.6797\n",
      "Epoch 30/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6040 - accuracy: 0.6387 - val_loss: 0.5901 - val_accuracy: 0.6797\n",
      "Epoch 31/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6018 - accuracy: 0.6387 - val_loss: 0.5893 - val_accuracy: 0.6797\n",
      "Epoch 32/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6007 - accuracy: 0.6387 - val_loss: 0.5893 - val_accuracy: 0.6797\n",
      "Epoch 33/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6007 - accuracy: 0.6387 - val_loss: 0.5885 - val_accuracy: 0.6797\n",
      "Epoch 34/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.5999 - accuracy: 0.6387 - val_loss: 0.5879 - val_accuracy: 0.6797\n",
      "Epoch 35/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.5986 - accuracy: 0.6387 - val_loss: 0.5886 - val_accuracy: 0.6797\n",
      "Epoch 36/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.5979 - accuracy: 0.6387 - val_loss: 0.5872 - val_accuracy: 0.6797\n",
      "Epoch 37/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.5976 - accuracy: 0.6387 - val_loss: 0.5867 - val_accuracy: 0.6797\n",
      "Epoch 38/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.5965 - accuracy: 0.6387 - val_loss: 0.5859 - val_accuracy: 0.6797\n",
      "Epoch 39/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.5955 - accuracy: 0.6387 - val_loss: 0.5870 - val_accuracy: 0.6797\n",
      "Epoch 40/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.5942 - accuracy: 0.6387 - val_loss: 0.5844 - val_accuracy: 0.6797\n",
      "Epoch 41/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.5936 - accuracy: 0.6387 - val_loss: 0.5850 - val_accuracy: 0.6797\n",
      "Epoch 42/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.5923 - accuracy: 0.6387 - val_loss: 0.5839 - val_accuracy: 0.6797\n",
      "Epoch 43/50\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.5934 - accuracy: 0.6387 - val_loss: 0.5842 - val_accuracy: 0.6797\n",
      "Epoch 44/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.5910 - accuracy: 0.6387 - val_loss: 0.5848 - val_accuracy: 0.6797\n",
      "Epoch 45/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.5905 - accuracy: 0.6387 - val_loss: 0.5828 - val_accuracy: 0.6797\n",
      "Epoch 46/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.5900 - accuracy: 0.6387 - val_loss: 0.5823 - val_accuracy: 0.6797\n",
      "Epoch 47/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.5920 - accuracy: 0.6387 - val_loss: 0.5810 - val_accuracy: 0.6797\n",
      "Epoch 48/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.5888 - accuracy: 0.6387 - val_loss: 0.5814 - val_accuracy: 0.6797\n",
      "Epoch 49/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.5883 - accuracy: 0.6387 - val_loss: 0.5811 - val_accuracy: 0.6797\n",
      "Epoch 50/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.5872 - accuracy: 0.6387 - val_loss: 0.5863 - val_accuracy: 0.6797\n",
      "8/8 [==============================] - 0s 1ms/step\n",
      "Epoch 1/50\n",
      "17/17 [==============================] - 1s 18ms/step - loss: 0.5868 - accuracy: 0.6387 - val_loss: 0.5815 - val_accuracy: 0.6797\n",
      "Epoch 2/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.5855 - accuracy: 0.6387 - val_loss: 0.5819 - val_accuracy: 0.6797\n",
      "Epoch 3/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.5854 - accuracy: 0.6387 - val_loss: 0.5819 - val_accuracy: 0.6797\n",
      "Epoch 4/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.5853 - accuracy: 0.6387 - val_loss: 0.5814 - val_accuracy: 0.6797\n",
      "Epoch 5/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.5852 - accuracy: 0.6387 - val_loss: 0.5816 - val_accuracy: 0.6797\n",
      "Epoch 6/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.5850 - accuracy: 0.6387 - val_loss: 0.5817 - val_accuracy: 0.6797\n",
      "Epoch 7/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.5851 - accuracy: 0.6387 - val_loss: 0.5816 - val_accuracy: 0.6797\n",
      "Epoch 8/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.5850 - accuracy: 0.6387 - val_loss: 0.5815 - val_accuracy: 0.6797\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.5849 - accuracy: 0.6387 - val_loss: 0.5817 - val_accuracy: 0.6797\n",
      "Epoch 10/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.5849 - accuracy: 0.6387 - val_loss: 0.5816 - val_accuracy: 0.6797\n",
      "Epoch 11/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.5849 - accuracy: 0.6387 - val_loss: 0.5816 - val_accuracy: 0.6797\n",
      "Epoch 12/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.5849 - accuracy: 0.6387 - val_loss: 0.5816 - val_accuracy: 0.6797\n",
      "Epoch 13/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.5847 - accuracy: 0.6387 - val_loss: 0.5816 - val_accuracy: 0.6797\n",
      "Epoch 14/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.5847 - accuracy: 0.6387 - val_loss: 0.5817 - val_accuracy: 0.6797\n",
      "Epoch 15/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.5848 - accuracy: 0.6387 - val_loss: 0.5817 - val_accuracy: 0.6797\n",
      "Epoch 16/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.5847 - accuracy: 0.6387 - val_loss: 0.5817 - val_accuracy: 0.6797\n",
      "Epoch 17/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.5847 - accuracy: 0.6387 - val_loss: 0.5816 - val_accuracy: 0.6797\n",
      "Epoch 18/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.5846 - accuracy: 0.6387 - val_loss: 0.5818 - val_accuracy: 0.6797\n",
      "Epoch 19/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.5846 - accuracy: 0.6387 - val_loss: 0.5817 - val_accuracy: 0.6797\n",
      "Epoch 20/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.5847 - accuracy: 0.6387 - val_loss: 0.5816 - val_accuracy: 0.6797\n",
      "Epoch 21/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.5845 - accuracy: 0.6387 - val_loss: 0.5816 - val_accuracy: 0.6797\n",
      "Epoch 22/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.5846 - accuracy: 0.6387 - val_loss: 0.5816 - val_accuracy: 0.6797\n",
      "Epoch 23/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.5845 - accuracy: 0.6387 - val_loss: 0.5816 - val_accuracy: 0.6797\n",
      "Epoch 24/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.5845 - accuracy: 0.6387 - val_loss: 0.5816 - val_accuracy: 0.6797\n",
      "Epoch 25/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.5844 - accuracy: 0.6387 - val_loss: 0.5815 - val_accuracy: 0.6797\n",
      "Epoch 26/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.5845 - accuracy: 0.6387 - val_loss: 0.5816 - val_accuracy: 0.6797\n",
      "Epoch 27/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.5843 - accuracy: 0.6387 - val_loss: 0.5816 - val_accuracy: 0.6797\n",
      "Epoch 28/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.5843 - accuracy: 0.6387 - val_loss: 0.5816 - val_accuracy: 0.6797\n",
      "Epoch 29/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.5844 - accuracy: 0.6387 - val_loss: 0.5816 - val_accuracy: 0.6797\n",
      "Epoch 30/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.5843 - accuracy: 0.6387 - val_loss: 0.5815 - val_accuracy: 0.6797\n",
      "Epoch 31/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.5843 - accuracy: 0.6387 - val_loss: 0.5815 - val_accuracy: 0.6797\n",
      "Epoch 32/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.5843 - accuracy: 0.6387 - val_loss: 0.5815 - val_accuracy: 0.6797\n",
      "Epoch 33/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.5843 - accuracy: 0.6387 - val_loss: 0.5816 - val_accuracy: 0.6797\n",
      "Epoch 34/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.5842 - accuracy: 0.6387 - val_loss: 0.5816 - val_accuracy: 0.6797\n",
      "Epoch 35/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.5841 - accuracy: 0.6387 - val_loss: 0.5816 - val_accuracy: 0.6797\n",
      "Epoch 36/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.5841 - accuracy: 0.6387 - val_loss: 0.5816 - val_accuracy: 0.6797\n",
      "Epoch 37/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.5842 - accuracy: 0.6387 - val_loss: 0.5816 - val_accuracy: 0.6797\n",
      "Epoch 38/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.5841 - accuracy: 0.6387 - val_loss: 0.5815 - val_accuracy: 0.6797\n",
      "Epoch 39/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.5841 - accuracy: 0.6387 - val_loss: 0.5816 - val_accuracy: 0.6797\n",
      "Epoch 40/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.5841 - accuracy: 0.6387 - val_loss: 0.5815 - val_accuracy: 0.6797\n",
      "Epoch 41/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.5841 - accuracy: 0.6387 - val_loss: 0.5815 - val_accuracy: 0.6797\n",
      "Epoch 42/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.5841 - accuracy: 0.6387 - val_loss: 0.5816 - val_accuracy: 0.6797\n",
      "Epoch 43/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.5841 - accuracy: 0.6387 - val_loss: 0.5816 - val_accuracy: 0.6797\n",
      "Epoch 44/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.5840 - accuracy: 0.6387 - val_loss: 0.5816 - val_accuracy: 0.6797\n",
      "Epoch 45/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.5839 - accuracy: 0.6387 - val_loss: 0.5816 - val_accuracy: 0.6797\n",
      "Epoch 46/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.5840 - accuracy: 0.6387 - val_loss: 0.5816 - val_accuracy: 0.6797\n",
      "Epoch 47/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.5840 - accuracy: 0.6387 - val_loss: 0.5815 - val_accuracy: 0.6797\n",
      "Epoch 48/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.5839 - accuracy: 0.6387 - val_loss: 0.5815 - val_accuracy: 0.6797\n",
      "Epoch 49/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.5840 - accuracy: 0.6387 - val_loss: 0.5814 - val_accuracy: 0.6797\n",
      "Epoch 50/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.5839 - accuracy: 0.6387 - val_loss: 0.5815 - val_accuracy: 0.6797\n",
      "8/8 [==============================] - 0s 2ms/step\n",
      "Epoch 1/50\n",
      "17/17 [==============================] - 1s 11ms/step - loss: 0.5977 - accuracy: 0.6387 - val_loss: 0.6116 - val_accuracy: 0.6797\n",
      "Epoch 2/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6018 - accuracy: 0.6387 - val_loss: 0.5798 - val_accuracy: 0.6797\n",
      "Epoch 3/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.5931 - accuracy: 0.6387 - val_loss: 0.5848 - val_accuracy: 0.6797\n",
      "Epoch 4/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.5963 - accuracy: 0.6387 - val_loss: 0.6021 - val_accuracy: 0.6797\n",
      "Epoch 5/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6038 - accuracy: 0.6387 - val_loss: 0.5851 - val_accuracy: 0.6797\n",
      "Epoch 6/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.5975 - accuracy: 0.6387 - val_loss: 0.5848 - val_accuracy: 0.6797\n",
      "Epoch 7/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.5971 - accuracy: 0.6387 - val_loss: 0.5959 - val_accuracy: 0.6797\n",
      "Epoch 8/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.5969 - accuracy: 0.6387 - val_loss: 0.5907 - val_accuracy: 0.6797\n",
      "Epoch 9/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.5936 - accuracy: 0.6387 - val_loss: 0.5809 - val_accuracy: 0.6797\n",
      "Epoch 10/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.5933 - accuracy: 0.6387 - val_loss: 0.5987 - val_accuracy: 0.6797\n",
      "Epoch 11/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.5891 - accuracy: 0.6387 - val_loss: 0.5831 - val_accuracy: 0.6797\n",
      "Epoch 12/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.5923 - accuracy: 0.6387 - val_loss: 0.5963 - val_accuracy: 0.6797\n",
      "Epoch 13/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.5962 - accuracy: 0.6387 - val_loss: 0.5806 - val_accuracy: 0.6797\n",
      "Epoch 14/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.5902 - accuracy: 0.6387 - val_loss: 0.5806 - val_accuracy: 0.6797\n",
      "Epoch 15/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.5989 - accuracy: 0.6387 - val_loss: 0.5849 - val_accuracy: 0.6797\n",
      "Epoch 16/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.5989 - accuracy: 0.6387 - val_loss: 0.5828 - val_accuracy: 0.6797\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.5942 - accuracy: 0.6387 - val_loss: 0.5932 - val_accuracy: 0.6797\n",
      "Epoch 18/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.5942 - accuracy: 0.6387 - val_loss: 0.5841 - val_accuracy: 0.6797\n",
      "Epoch 19/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.5898 - accuracy: 0.6387 - val_loss: 0.5914 - val_accuracy: 0.6797\n",
      "Epoch 20/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6002 - accuracy: 0.6387 - val_loss: 0.5800 - val_accuracy: 0.6797\n",
      "Epoch 21/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.5932 - accuracy: 0.6387 - val_loss: 0.5840 - val_accuracy: 0.6797\n",
      "Epoch 22/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.5934 - accuracy: 0.6387 - val_loss: 0.5841 - val_accuracy: 0.6797\n",
      "Epoch 23/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.5895 - accuracy: 0.6387 - val_loss: 0.5808 - val_accuracy: 0.6797\n",
      "Epoch 24/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.5909 - accuracy: 0.6387 - val_loss: 0.5869 - val_accuracy: 0.6797\n",
      "Epoch 25/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.5924 - accuracy: 0.6387 - val_loss: 0.5926 - val_accuracy: 0.6797\n",
      "Epoch 26/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.5976 - accuracy: 0.6387 - val_loss: 0.5814 - val_accuracy: 0.6797\n",
      "Epoch 27/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.5937 - accuracy: 0.6387 - val_loss: 0.5810 - val_accuracy: 0.6797\n",
      "Epoch 28/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6044 - accuracy: 0.6387 - val_loss: 0.5801 - val_accuracy: 0.6797\n",
      "Epoch 29/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.5909 - accuracy: 0.6387 - val_loss: 0.5802 - val_accuracy: 0.6797\n",
      "Epoch 30/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.5893 - accuracy: 0.6387 - val_loss: 0.5804 - val_accuracy: 0.6797\n",
      "Epoch 31/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.5896 - accuracy: 0.6387 - val_loss: 0.5801 - val_accuracy: 0.6797\n",
      "Epoch 32/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.5861 - accuracy: 0.6387 - val_loss: 0.5800 - val_accuracy: 0.6797\n",
      "Epoch 33/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.5956 - accuracy: 0.6387 - val_loss: 0.5835 - val_accuracy: 0.6797\n",
      "Epoch 34/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.5924 - accuracy: 0.6387 - val_loss: 0.5820 - val_accuracy: 0.6797\n",
      "Epoch 35/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.5983 - accuracy: 0.6387 - val_loss: 0.6084 - val_accuracy: 0.6797\n",
      "Epoch 36/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.5999 - accuracy: 0.6387 - val_loss: 0.5876 - val_accuracy: 0.6797\n",
      "Epoch 37/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.5959 - accuracy: 0.6387 - val_loss: 0.5818 - val_accuracy: 0.6797\n",
      "Epoch 38/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.5868 - accuracy: 0.6387 - val_loss: 0.5802 - val_accuracy: 0.6797\n",
      "Epoch 39/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.5922 - accuracy: 0.6387 - val_loss: 0.5845 - val_accuracy: 0.6797\n",
      "Epoch 40/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.5847 - accuracy: 0.6387 - val_loss: 0.5804 - val_accuracy: 0.6797\n",
      "Epoch 41/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.5894 - accuracy: 0.6387 - val_loss: 0.5862 - val_accuracy: 0.6797\n",
      "Epoch 42/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.5899 - accuracy: 0.6387 - val_loss: 0.5819 - val_accuracy: 0.6797\n",
      "Epoch 43/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.5893 - accuracy: 0.6387 - val_loss: 0.5976 - val_accuracy: 0.6797\n",
      "Epoch 44/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.5871 - accuracy: 0.6387 - val_loss: 0.5852 - val_accuracy: 0.6797\n",
      "Epoch 45/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.5993 - accuracy: 0.6387 - val_loss: 0.5799 - val_accuracy: 0.6797\n",
      "Epoch 46/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.5896 - accuracy: 0.6387 - val_loss: 0.5865 - val_accuracy: 0.6797\n",
      "Epoch 47/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.5897 - accuracy: 0.6387 - val_loss: 0.6075 - val_accuracy: 0.6797\n",
      "Epoch 48/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6041 - accuracy: 0.6387 - val_loss: 0.5847 - val_accuracy: 0.6797\n",
      "Epoch 49/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.5926 - accuracy: 0.6387 - val_loss: 0.5813 - val_accuracy: 0.6797\n",
      "Epoch 50/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.5848 - accuracy: 0.6387 - val_loss: 0.5862 - val_accuracy: 0.6797\n",
      "8/8 [==============================] - 0s 2ms/step\n",
      "Epoch 1/50\n",
      "17/17 [==============================] - 2s 23ms/step - loss: 0.5956 - accuracy: 0.6387 - val_loss: 0.5861 - val_accuracy: 0.6797\n",
      "Epoch 2/50\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.5956 - accuracy: 0.6387 - val_loss: 0.5860 - val_accuracy: 0.6797\n",
      "Epoch 3/50\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 0.5955 - accuracy: 0.6387 - val_loss: 0.5860 - val_accuracy: 0.6797\n",
      "Epoch 4/50\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.5955 - accuracy: 0.6387 - val_loss: 0.5859 - val_accuracy: 0.6797\n",
      "Epoch 5/50\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.5954 - accuracy: 0.6387 - val_loss: 0.5859 - val_accuracy: 0.6797\n",
      "Epoch 6/50\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.5954 - accuracy: 0.6387 - val_loss: 0.5858 - val_accuracy: 0.6797\n",
      "Epoch 7/50\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.5954 - accuracy: 0.6387 - val_loss: 0.5858 - val_accuracy: 0.6797\n",
      "Epoch 8/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.5953 - accuracy: 0.6387 - val_loss: 0.5857 - val_accuracy: 0.6797\n",
      "Epoch 9/50\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.5953 - accuracy: 0.6387 - val_loss: 0.5857 - val_accuracy: 0.6797\n",
      "Epoch 10/50\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.5952 - accuracy: 0.6387 - val_loss: 0.5856 - val_accuracy: 0.6797\n",
      "Epoch 11/50\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 0.5952 - accuracy: 0.6387 - val_loss: 0.5856 - val_accuracy: 0.6797\n",
      "Epoch 12/50\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 0.5952 - accuracy: 0.6387 - val_loss: 0.5856 - val_accuracy: 0.6797\n",
      "Epoch 13/50\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.5951 - accuracy: 0.6387 - val_loss: 0.5855 - val_accuracy: 0.6797\n",
      "Epoch 14/50\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.5951 - accuracy: 0.6387 - val_loss: 0.5855 - val_accuracy: 0.6797\n",
      "Epoch 15/50\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.5951 - accuracy: 0.6387 - val_loss: 0.5854 - val_accuracy: 0.6797\n",
      "Epoch 16/50\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.5950 - accuracy: 0.6387 - val_loss: 0.5854 - val_accuracy: 0.6797\n",
      "Epoch 17/50\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.5950 - accuracy: 0.6387 - val_loss: 0.5854 - val_accuracy: 0.6797\n",
      "Epoch 18/50\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.5950 - accuracy: 0.6387 - val_loss: 0.5853 - val_accuracy: 0.6797\n",
      "Epoch 19/50\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.5949 - accuracy: 0.6387 - val_loss: 0.5853 - val_accuracy: 0.6797\n",
      "Epoch 20/50\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.5949 - accuracy: 0.6387 - val_loss: 0.5852 - val_accuracy: 0.6797\n",
      "Epoch 21/50\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.5949 - accuracy: 0.6387 - val_loss: 0.5852 - val_accuracy: 0.6797\n",
      "Epoch 22/50\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.5948 - accuracy: 0.6387 - val_loss: 0.5852 - val_accuracy: 0.6797\n",
      "Epoch 23/50\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.5948 - accuracy: 0.6387 - val_loss: 0.5851 - val_accuracy: 0.6797\n",
      "Epoch 24/50\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 0.5948 - accuracy: 0.6387 - val_loss: 0.5851 - val_accuracy: 0.6797\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/50\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 0.5947 - accuracy: 0.6387 - val_loss: 0.5851 - val_accuracy: 0.6797\n",
      "Epoch 26/50\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.5947 - accuracy: 0.6387 - val_loss: 0.5850 - val_accuracy: 0.6797\n",
      "Epoch 27/50\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.5947 - accuracy: 0.6387 - val_loss: 0.5850 - val_accuracy: 0.6797\n",
      "Epoch 28/50\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.5946 - accuracy: 0.6387 - val_loss: 0.5850 - val_accuracy: 0.6797\n",
      "Epoch 29/50\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.5946 - accuracy: 0.6387 - val_loss: 0.5849 - val_accuracy: 0.6797\n",
      "Epoch 30/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.5945 - accuracy: 0.6387 - val_loss: 0.5849 - val_accuracy: 0.6797\n",
      "Epoch 31/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.5945 - accuracy: 0.6387 - val_loss: 0.5849 - val_accuracy: 0.6797\n",
      "Epoch 32/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.5945 - accuracy: 0.6387 - val_loss: 0.5848 - val_accuracy: 0.6797\n",
      "Epoch 33/50\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.5944 - accuracy: 0.6387 - val_loss: 0.5848 - val_accuracy: 0.6797\n",
      "Epoch 34/50\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.5944 - accuracy: 0.6387 - val_loss: 0.5847 - val_accuracy: 0.6797\n",
      "Epoch 35/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.5944 - accuracy: 0.6387 - val_loss: 0.5847 - val_accuracy: 0.6797\n",
      "Epoch 36/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.5943 - accuracy: 0.6387 - val_loss: 0.5847 - val_accuracy: 0.6797\n",
      "Epoch 37/50\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.5943 - accuracy: 0.6387 - val_loss: 0.5847 - val_accuracy: 0.6797\n",
      "Epoch 38/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.5943 - accuracy: 0.6387 - val_loss: 0.5846 - val_accuracy: 0.6797\n",
      "Epoch 39/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.5942 - accuracy: 0.6387 - val_loss: 0.5846 - val_accuracy: 0.6797\n",
      "Epoch 40/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.5942 - accuracy: 0.6387 - val_loss: 0.5846 - val_accuracy: 0.6797\n",
      "Epoch 41/50\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.5942 - accuracy: 0.6387 - val_loss: 0.5845 - val_accuracy: 0.6797\n",
      "Epoch 42/50\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.5941 - accuracy: 0.6387 - val_loss: 0.5845 - val_accuracy: 0.6797\n",
      "Epoch 43/50\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.5941 - accuracy: 0.6387 - val_loss: 0.5845 - val_accuracy: 0.6797\n",
      "Epoch 44/50\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.5941 - accuracy: 0.6387 - val_loss: 0.5844 - val_accuracy: 0.6797\n",
      "Epoch 45/50\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.5940 - accuracy: 0.6387 - val_loss: 0.5844 - val_accuracy: 0.6797\n",
      "Epoch 46/50\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.5940 - accuracy: 0.6387 - val_loss: 0.5844 - val_accuracy: 0.6797\n",
      "Epoch 47/50\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.5940 - accuracy: 0.6387 - val_loss: 0.5844 - val_accuracy: 0.6797\n",
      "Epoch 48/50\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.5940 - accuracy: 0.6387 - val_loss: 0.5843 - val_accuracy: 0.6797\n",
      "Epoch 49/50\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.5939 - accuracy: 0.6387 - val_loss: 0.5843 - val_accuracy: 0.6797\n",
      "Epoch 50/50\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.5939 - accuracy: 0.6387 - val_loss: 0.5843 - val_accuracy: 0.6797\n",
      "8/8 [==============================] - 0s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "# Comparaisons des optimisateurs\n",
    "opt= ['adam','adagrad','sgd','adadelta']\n",
    "acc=[]\n",
    "for i in range(0,len(opt)) :\n",
    "    model.compile(optimizer=opt[i],loss='binary_crossentropy',metrics=['accuracy'])\n",
    "    model.fit(x=X_train, y=y_train, validation_data=(X_test, y_test), epochs=50)\n",
    "    pred_model = model.predict(X_test)\n",
    "    acc.append(opt[i]+' ' +str(accuracy_score(y_test, pred_model.round())))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "f4715ec6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['adam 0.6796536796536796',\n",
       " 'adagrad 0.6796536796536796',\n",
       " 'sgd 0.4155844155844156',\n",
       " 'adadelta 0.43722943722943725']"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18173402",
   "metadata": {},
   "source": [
    "Nous constatons que Adam et Adagrad donnent les plus grandes accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c32200",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "En résumé, l'optimiseur Adagrad est un choix solide pour l'optimisation en Deep Learning, offrant des avantages tels que l'adaptation automatique du taux d'apprentissage et la gestion des problèmes de données rares. Cependant, il est important de surveiller attentivement son comportement pour éviter les problèmes de convergence lente ou de diminution du taux d'apprentissage. En fin de compte, le choix de l'optimiseur dépendra du problème spécifique et des caractéristiques du jeu de données, et il peut être utile d'expérimenter différentes options pour trouver la meilleure solution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93953212",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "512c6ce2",
   "metadata": {},
   "source": [
    "#          Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ac2f92",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82822df9",
   "metadata": {},
   "source": [
    "Gradient Descent est un processus d'optimisation itératif qui recherche la valeur optimale d'une fonction objective (Minimum/Maximum). C'est l'une des méthodes les plus utilisées pour modifier les paramètres d'un modèle afin de réduire une fonction de coût dans les projets d'apprentissage automatique. \n",
    "L'objectif principale de la descente de gradient est d'identifier les paramètres du modele qui fournissent la précision maximale sur les ensembles de données d'entrainement et de test. En descente de grandient, le gradient est un vecteur pointant dans la direction générale de la montée la plus raide de la fonction en un point particulier. L'algorithm peut progressivement descendre vers des valeurs inférieures de la fonction en se déplaçant dans le sens opposé du grandient, jusqu'à atteindre le minimum de la fonction. \n",
    "Il en existe trois parmi lesquels le SGD qui est l'objet de notre exposé."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a25fdc7",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent (SGD)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d235ff8",
   "metadata": {},
   "source": [
    "L'optimiseur SGD (Stochastic Gradient Descent) est un algorithme couramment utilisé pour l'entraînement des réseaux de neurones dans le domaine de l'apprentissage automatique (machine learning). L'objectif principal de SGD est de minimiser une fonction de perte en ajustant les poids du modèle à chaque itération.\n",
    "\n",
    "Le principe de base de SGD consiste à estimer le gradient de la fonction de perte en utilisant un sous-ensemble aléatoire des données d'entraînement à chaque itération. Plutôt que de calculer le gradient sur l'ensemble complet des données d'entraînement, SGD utilise une approche stochastique où il sélectionne un échantillon aléatoire à chaque itération. Cela rend l'algorithme plus rapide en termes de calculs, mais il peut également être plus bruité et moins précis que les méthodes d'optimisation déterministes.\n",
    "Le processus de mise à jour des poids dans SGD peut être décrit comme suit :\n",
    "1. Initialisation : Les poids du modèle sont initialisés avec des valeurs aléatoires ou prédéfinies.\n",
    "2. Sélection d'un échantillon : Un petit sous-ensemble d'exemples d'entraînement est sélectionné aléatoirement à partir de l'ensemble complet des données d'entraînement.\n",
    "3. Calcul du gradient : Le gradient de la fonction de perte par rapport aux poids est calculé à l'aide de l'échantillon sélectionné.\n",
    "4. Mise à jour des poids : Les poids du modèle sont mis à jour en fonction du gradient calculé. La mise à jour peut être effectuée selon différentes variantes de SGD, telles que la descente de gradient standard, la descente de gradient avec momentum, ou la descente de gradient avec learning rate adaptatif.\n",
    "5. Répétition : Les étapes 2 à 4 sont répétées jusqu'à ce qu'un critère d'arrêt soit atteint, par exemple un nombre fixe d'itérations ou une convergence de la fonction de perte.\n",
    "\n",
    "L'optimiseur SGD présente plusieurs avantages. Il est relativement simple à mettre en œuvre et à utiliser, et il fonctionne bien pour de nombreux problèmes d'apprentissage automatique. De plus, en utilisant des échantillons aléatoires pour estimer le gradient, SGD peut éviter de se retrouver coincé dans des minima locaux indésirables et peut être plus efficace pour les ensembles de données massifs.\n",
    "\n",
    "Cependant, SGD présente également quelques limitations. En raison de son approche stochastique, il peut être plus difficile à converger vers un minimum global de la fonction de perte, en particulier pour des ensembles de données complexes ou avec des surfaces de perte non convexes. De plus, il peut être sensible au choix du learning rate et peut nécessiter un réglage fin pour obtenir de bonnes performances.\n",
    "\n",
    "Dans la pratique, plusieurs variantes de SGD ont été développées pour améliorer ses performances, notamment l'ajout de termes de momentum, l'adaptation du learning rate ou l'utilisation de méthodes de descente de gradient plus sophistiquées comme Adam (Adaptive Moment Estimation). Ces améliorations permettent de surmonter certaines des limitations de SGD et d'accélérer la convergence vers des modèles optimaux.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab4e7c9",
   "metadata": {},
   "source": [
    "## AVANTAGES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8313af8",
   "metadata": {},
   "source": [
    "- plus rapide que la SGD classique,\n",
    "- s'adapte rapidement aux variations de données"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd477d0",
   "metadata": {},
   "source": [
    "## INCONVENIENTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e89f16b",
   "metadata": {},
   "source": [
    "- Elle peut être sujette au bruit et à la variabilité des estimation du gradient,\n",
    "- Elle peut avoir du mal à atteindre le min globale de la fonction de perte."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1cabf61",
   "metadata": {},
   "source": [
    "## APPLICATION "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53b1bba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    " \n",
    "class SGD:\n",
    "    def __init__(self, lr=0.01, max_iter=1000, batch_size=32, tol=1e-3):\n",
    "        # learning rate of the SGD Optimizer\n",
    "        self.learning_rate = lr\n",
    "        # maximum number of iterations for SGD Optimizer\n",
    "        self.max_iteration = max_iter\n",
    "        # mini-batch size of the data\n",
    "        self.batch_size = batch_size \n",
    "        # tolerance for convergence for the theta\n",
    "        self.tolerence_convergence  = tol \n",
    "        # Initialize model parameters to None\n",
    "        self.theta = None \n",
    "         \n",
    "    def fit(self, X, y):\n",
    "        # store dimension of input vector\n",
    "        n, d = X.shape\n",
    "        # Intialize random Theta for every feature\n",
    "        self.theta = np.random.randn(d)\n",
    "        for i in range(self.max_iteration):\n",
    "            # Shuffle the data\n",
    "            indices = np.random.permutation(n)\n",
    "            X = X[indices]\n",
    "            y = y[indices]\n",
    "            # Iterate over mini-batches\n",
    "            for i in range(0, n, self.batch_size):\n",
    "                X_batch = X[i:i+self.batch_size]\n",
    "                y_batch = y[i:i+self.batch_size]\n",
    "                grad = self.gradient(X_batch, y_batch)\n",
    "                self.theta -= self.learning_rate * grad\n",
    "            # Check for convergence\n",
    "            if np.linalg.norm(grad) < self.tolerence_convergence:\n",
    "                break\n",
    "    # define a gradient functon for calculating gradient\n",
    "    # of the data\n",
    "    def gradient(self, X, y):\n",
    "        n = len(y)\n",
    "        # predict target value by taking taking\n",
    "        # taking dot product of dependent and theta value\n",
    "        y_pred = np.dot(X, self.theta)\n",
    "         \n",
    "        # calculate error between predict and actual value\n",
    "        error = y_pred - y\n",
    "        grad = np.dot(X.T, error) / n\n",
    "        return grad\n",
    "     \n",
    "    def predict(self, X):\n",
    "        # prdict y value using calculated theta value\n",
    "        y_pred = np.dot(X, self.theta)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "53b2440c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import des modules\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7ffa76d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('archive/fashion-mnist_train.csv');\n",
    "Y_train = df_train['label']\n",
    "X_train = df_train.drop('label', axis =1)\n",
    "df_test = pd.read_csv('archive/fashion-mnist_test.csv');\n",
    "Y_test = df_test['label']\n",
    "X_test = df_test.drop('label', axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "796f6177",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prétraitement des données\n",
    "X_train = X_train / 255.0\n",
    "X_test = X_test / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6b7fa9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construction du modèle\n",
    "model = Sequential([\n",
    "    Dense(128, activation='relu',input_dim=784),\n",
    "    Dense(10, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "69ec2871",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définition de la fonction de perte et de l'optimiseur SGD\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7258c2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compilation du modèle\n",
    "model.compile(optimizer=optimizer, loss=loss_fn, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "864fdc11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1688/1688 [==============================] - 5s 2ms/step - loss: 0.7655 - accuracy: 0.7511 - val_loss: 0.5938 - val_accuracy: 0.8022\n",
      "Epoch 2/10\n",
      "1688/1688 [==============================] - 4s 3ms/step - loss: 0.5258 - accuracy: 0.8222 - val_loss: 0.5075 - val_accuracy: 0.8292\n",
      "Epoch 3/10\n",
      "1688/1688 [==============================] - 4s 3ms/step - loss: 0.4779 - accuracy: 0.8368 - val_loss: 0.4934 - val_accuracy: 0.8298\n",
      "Epoch 4/10\n",
      "1688/1688 [==============================] - 4s 2ms/step - loss: 0.4526 - accuracy: 0.8432 - val_loss: 0.4604 - val_accuracy: 0.8443\n",
      "Epoch 5/10\n",
      "1688/1688 [==============================] - 4s 2ms/step - loss: 0.4336 - accuracy: 0.8494 - val_loss: 0.4444 - val_accuracy: 0.8463\n",
      "Epoch 6/10\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.4193 - accuracy: 0.8535 - val_loss: 0.4438 - val_accuracy: 0.8477\n",
      "Epoch 7/10\n",
      "1688/1688 [==============================] - 4s 3ms/step - loss: 0.4071 - accuracy: 0.8578 - val_loss: 0.4370 - val_accuracy: 0.8485\n",
      "Epoch 8/10\n",
      "1688/1688 [==============================] - 4s 2ms/step - loss: 0.3968 - accuracy: 0.8617 - val_loss: 0.4255 - val_accuracy: 0.8568\n",
      "Epoch 9/10\n",
      "1688/1688 [==============================] - 5s 3ms/step - loss: 0.3877 - accuracy: 0.8643 - val_loss: 0.4183 - val_accuracy: 0.8582\n",
      "Epoch 10/10\n",
      "1688/1688 [==============================] - 4s 2ms/step - loss: 0.3805 - accuracy: 0.8670 - val_loss: 0.4078 - val_accuracy: 0.8598\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa2f578a760>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Entraînement du modèle avec SGD\n",
    "model.fit(X_train, Y_train, epochs=10, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "29f0c743",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 2ms/step - loss: 0.3899 - accuracy: 0.8643\n",
      "Test Loss: 0.38993775844573975\n",
      "Test Accuracy: 0.864300012588501\n"
     ]
    }
   ],
   "source": [
    "# Évaluation finale sur l'ensemble de test\n",
    "test_loss, test_accuracy = model.evaluate(X_test, Y_test)\n",
    "print(\"Test Loss:\", test_loss)\n",
    "print(\"Test Accuracy:\", test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d818eb98",
   "metadata": {},
   "source": [
    "Dans cet exemple, nous utilisons la bibliothèque TensorFlow pour charger le jeu de données Fashion-MNIST.\n",
    "Ensuite, nous prétraitons les images en les normalisant entre 0 et 1. \n",
    "Nous construisons un modèle séquentiel simple avec une couche dense de 128 neurones et une couche de sortie softmax \n",
    "avec 10 neurones correspondant aux 10 classes de Fashion-MNIST.\n",
    "\n",
    "Ensuite, nous définissons la fonction de perte (Cross-entropy) et l'optimiseur SGD avec un learning rate de 0.01.\n",
    "Nous compilons le modèle en spécifiant l'optimiseur et la fonction de perte.\n",
    "\n",
    "Ensuite, nous entraînons le modèle en utilisant la méthode fit en fournissant les données d'entraînement, le nombre \n",
    "d'époques, la taille des lots (batch size) et une validation split de 0.1 pour évaluer les performances du modèle pendant l'entraînement.\n",
    "\n",
    "Enfin, nous évaluons le modèle sur l'ensemble de test et affichons la perte et l'exactitude (accuracy) obtenues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c67593",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
